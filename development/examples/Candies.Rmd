---
title: "Candy example"
author: "Liland"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(HDANOVA, quietly = TRUE)
library(mixlm, quietly = TRUE)
```

## Data set

The candy dataset contains sensory assessments of 5 different candies by 11 trained assessors. Each assessor has judged 9 sensory attributes 3 times for each candy on a scale from 0 to 15. The dataset has 165 rows ($5 \times 11 \times 3$) and 2 factors: assessor and candy.

## Linear Mixed Model ANOVA

When analysing the candy dataset using Linear Mixed Model ANOVA (LMM), we focus on one sensory attribute and consider the assessors as a random effect. 

```{r cars}
# Load dataset and select first attribute
data(candies)
candies$transparent <- candies$assessment[,1]

# LMM
lmm <- lm(transparent ~ candy + r(assessor) + r(candy:assessor), 
          data = candies, unrestricted = FALSE)
Anova(lmm, type="III")
```

We observe significant effects of both candy and assessor with P-values < 0.01 and an interaction effect which would be significant at an \alpha level of 0.1. From this it is evident that there is great differences between the candies. In addition, the assessors have clear differences in their overall assessments, and to some degree also assess similar candies differently (the weak interaction).

```{r}
M <- matrix(0, 5, 11)
for(i in 1:11)
  M[,i] <- predict(lmm, data.frame(candy = factor(1:5), assessor = factor(1:11)[i]))
png("Candies.LMM.png", 1000,600, pointsize = 24)
par(mar=c(5,4,2,1), mfrow=c(1,1))
matplot(M, type="l", xlab = "candy", ylab = "transparent")
dev.off()
```

A plot of the predicted candy assessments with one line per assessor confirms the LMM results visually. We observe a general trend in assessments, but also different usage of the assessment scale (some spanning the full scale, some compressing their assessments) and some indidvidual differences for the candies.

```{r}
# Compact letter display
cld(simple.glht(lmm, "candy", corr = "Tukey"))
```

The compact letter display reveals two distinct groups of candies. The minimum significant difference of the candies is 1.9854 confirming the groupings found by Tukey's Honestly Significant Differences.


# Ellipsoids

The candy dataset contains sensory assessments of 5 different candies by 11 trained assessors. Each assessor has judged 9 sensory attributes 3 times for each candy on a scale from 0 to 15. The dataset has 165 rows ($5 \times 11 \times 3$) and 2 factors: assessor and candy.

We run two different models on the dataset, one with only main effects (assessment \~ assessor + candy), one also including interactions (assessment \~ assessor + candy + assessor:candy). The score plots in Figure \ref{FigCh04:ellipsoids} show factor level means for the candy factor and back-projected observations for the two models. On top of these we have added model ellipsoids and data ellipsoids.

The difference between data ellipsoids and model ellipsoids is evident as the model ellipsoids are all equal, except for location, and do not cover the variation of the back-projected points. We observe that the two candies on the left are significantly different from the candies on the right due to their ellipsoids not overlapping each other's mean value. The two candies on the left are also easily distinguishable, while three candies on the right are less obviously different. The effect of including the interaction effect in the model can be observed by the reduced spread in points and reduced sizes of the ellipsoids. 

```{r}
png("Ellipsoids_candies.png", 1200,1200, pointsize = 24)
par(mfrow=c(2,2), mar=c(3.5,3.5,4,2), mgp = c(2,0.75,0))
mod <- asca(assessment ~ assessor+candy, data = candies)
scoreplot(mod, factor = "candy", ellipsoids = "confidence", main = "Model: A + C",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
mod <- asca(assessment ~ assessor*candy, data = candies)
scoreplot(mod, factor = "candy", ellipsoids = "confidence", main = "Model: A + C + A:C",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
#L <- loadings(mod, factor = "candy")
#class(L) <- "scores"
#scoreplot(L, xlim=c(-0.6,0.6), labels = "names", panel.first = abline(v=0,h=0,lty=2, #col="gray"), cex = 0.8, main = "Loadings")
mod <- asca(assessment ~ assessor+candy, data = candies)
scoreplot(mod, factor = "candy", ellipsoids = "data", main = "Data: A + C",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
mod <- asca(assessment ~ assessor*candy, data = candies)
scoreplot(mod, factor = "candy", ellipsoids = "data", main = "Data: A + C + A:C",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
dev.off()

```

```{r}
pdf("Ellipsoids_candies_N321.pdf", 6.6,10)#, pointsize = 24)
#png("Ellipsoids_candies_N321.png", 1000,1500, pointsize = 24)
par(mfrow=c(3,2), mar=c(3.5,3.5,4,2), mgp = c(2,0.75,0))
mod <- asca(assessment ~ assessor+candy, data = candies)
scoreplot(mod, factor = "candy", ellipsoids = "confidence", main = "Model: A + C, N = 3",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
mod <- asca(assessment ~ assessor*candy, data = candies)
scoreplot(mod, factor = "candy", ellipsoids = "confidence", main = "Model: A + C + A:C, N = 3",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
#L <- loadings(mod, factor = "candy")
#class(L) <- "scores"
#scoreplot(L, xlim=c(-0.6,0.6), labels = "names", panel.first = abline(v=0,h=0,lty=2, #col="gray"), cex = 0.8, main = "Loadings")
mod <- asca(assessment ~ assessor+candy, data = candies)
scoreplot(mod, factor = "candy", ellipsoids = "data", main = "Data: A + C, N = 3",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
mod <- asca(assessment ~ assessor*candy, data = candies)
scoreplot(mod, factor = "candy", ellipsoids = "data", main = "Data: A + C + A:C, N = 3",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))

mod2 <- asca(assessment ~ assessor+candy, data = candies2)
scoreplot(mod2, factor = "candy", ellipsoids = "confidence", main = "Model: A + C, N = 2",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
mod3 <- asca(assessment ~ assessor+candy, data = candies3)
scoreplot(mod3, factor = "candy", ellipsoids = "confidence", main = "Model: A + C, N = 1",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
dev.off()
```

```{r}
par(mfrow=c(1,3), mar=c(3.5,3.5,4,2), mgp = c(2,0.75,0))
mod <- asca(assessment ~ assessor+candy, data = candies)
scoreplot(mod, factor = "candy", ellipsoids = "confidence", main = "Model: A + C, N = 3",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
candies2 <- candies[rep(1:3,55) < 3,]
mod2 <- asca(assessment ~ assessor+candy, data = candies2)
scoreplot(mod2, factor = "candy", ellipsoids = "confidence", main = "Model: A + C, N = 2",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
candies3 <- candies[rep(1:3,55) == 1,]
mod3 <- asca(assessment ~ assessor+candy, data = candies3)
scoreplot(mod3, factor = "candy", ellipsoids = "confidence", main = "Model: A + C, N = 1",
          panel.first = abline(v=0,h=0,lty=2, col="gray"), lwd=4,
          xlim = c(-26, 22), ylim = c(-15, 15))
```


```{r}
library(R.matlab)
dat <- readMat("./development/examples/Pasta_NIR.mat")
pasta <- data.frame(NIR = I(dat$xa), f1=factor(dat$desmat[,1]), f2=factor(dat$desmat[,2]), f3=factor(dat$desmat[,3]))
```

